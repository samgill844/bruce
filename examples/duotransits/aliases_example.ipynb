{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9847369",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3e19f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bruce, numpy as np, matplotlib.pyplot as plt, os\n",
    "from astropy.table import Table, Column\n",
    "from astropy.stats import sigma_clip\n",
    "from astropy.time import Time , TimeDelta\n",
    "from astropy import units as u\n",
    "import gc, multiprocess\n",
    "from scipy.stats import median_abs_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7fe390c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0606724",
   "metadata": {},
   "source": [
    "# Load the tics\n",
    "\n",
    "Data should have the form of (ascii or CSV, your choice). t_zero is in full BJD, width is in days, depth is in normalised flux. If you ue SPOCFIT, if you fit a single transit you will see 3 reported values on the bottom row which are these values for each event (t_zero, width, depth).\n",
    "\n",
    "tic_id,\tt_zero_1,\twidth_1,\tdepth_1,\tt_zero_2,\twidth_2,\tdepth_2,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ebbe077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  TIC ID  Linked TIC ID TOI ID   t_zero_1   width_1     depth_1    t_zero_2   width_2  depth_2 RA (decimal deg) Dec (decimal deg) ... Active_Aliases NGTS data? /comment Recent TESS sectors -check LC Upcoming TESS sectors RV - SolÃ¨ne notes Simbad Papers SG1 Published?   tic_id \n",
      "--------- ------------- ------ ----------- ---------- ----------- ---------- --------- ------- ---------------- ----------------- ... -------------- ------------------- ----------------------------- --------------------- ----------------- ------ ------ --- ---------- ---------\n",
      "257024338     257024338      - 2458922.486 0.10111937 0.006061971 2459411.07 0.1892989   0.004      225.6998706       76.65253491 ...             --                  --                             -           119,120,121                --     --     --  --         -- 257024338\n"
     ]
    }
   ],
   "source": [
    "duos = Table.read('duos.csv')\n",
    "duos['tic_id'] = duos['TIC ID']\n",
    "print(duos[duos['tic_id']==257024338])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae682c",
   "metadata": {},
   "source": [
    "# Now the main worker function\n",
    "\n",
    "This function does the following:\n",
    "-  Load the latest TESS data\n",
    "-  Flatten the lightcurve\n",
    "-  Fit the events\n",
    "-  Calcualte the aliases\n",
    "-  Plot the permissable aliases\n",
    "-  Create a report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6e4891b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‚ Download directory: /var/folders/fs/c4gpqhmx5tbcbsv_cbs0yhn80000gn/T/tmplfd_93s4\n",
      "\n",
      "Querying TIC 140215502 from MAST...\n",
      "Downloading URL https://mast.stsci.edu/api/v0.1/Download/file?uri=mast:HLSP/tess-spoc/s0005/target/0000/0001/4021/5502/hlsp_tess-spoc_tess_phot_0000000140215502-s0005_tess_v1_lc.fits to ./mastDownload/HLSP/hlsp_tess-spoc_tess_phot_0000000140215502-s0005_tess_v1_tp/hlsp_tess-spoc_tess_phot_0000000140215502-s0005_tess_v1_lc.fits ... [Done]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: UnitsWarning: 'BJD - 2457000, days' did not parse as fits unit: At col 0, Unit 'BJD' not supported by the FITS standard.  If this is meant to be a custom unit, define it with 'u.def_unit'. To have it recognized inside a file reader or other code, enable it with 'u.add_enabled_units'. For details, see https://docs.astropy.org/en/latest/units/combining_and_defining.html [astropy.units.core]\n",
      "WARNING: UnitsWarning: 'e-/s' did not parse as fits unit: At col 0, Unit 'e' not supported by the FITS standard.  If this is meant to be a custom unit, define it with 'u.def_unit'. To have it recognized inside a file reader or other code, enable it with 'u.add_enabled_units'. For details, see https://docs.astropy.org/en/latest/units/combining_and_defining.html [astropy.units.core]\n",
      "WARNING: UnitsWarning: 'pixels' did not parse as fits unit: At col 0, Unit 'pixels' not supported by the FITS standard. Did you mean pixel? If this is meant to be a custom unit, define it with 'u.def_unit'. To have it recognized inside a file reader or other code, enable it with 'u.add_enabled_units'. For details, see https://docs.astropy.org/en/latest/units/combining_and_defining.html [astropy.units.core]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading URL https://mast.stsci.edu/api/v0.1/Download/file?uri=mast:HLSP/tess-spoc/s0006/target/0000/0001/4021/5502/hlsp_tess-spoc_tess_phot_0000000140215502-s0006_tess_v1_lc.fits to ./mastDownload/HLSP/hlsp_tess-spoc_tess_phot_0000000140215502-s0006_tess_v1_tp/hlsp_tess-spoc_tess_phot_0000000140215502-s0006_tess_v1_lc.fits ... [Done]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: UnitsWarning: 'BJD - 2457000, days' did not parse as fits unit: At col 0, Unit 'BJD' not supported by the FITS standard.  If this is meant to be a custom unit, define it with 'u.def_unit'. To have it recognized inside a file reader or other code, enable it with 'u.add_enabled_units'. For details, see https://docs.astropy.org/en/latest/units/combining_and_defining.html [astropy.units.core]\n",
      "WARNING: UnitsWarning: 'e-/s' did not parse as fits unit: At col 0, Unit 'e' not supported by the FITS standard.  If this is meant to be a custom unit, define it with 'u.def_unit'. To have it recognized inside a file reader or other code, enable it with 'u.add_enabled_units'. For details, see https://docs.astropy.org/en/latest/units/combining_and_defining.html [astropy.units.core]\n",
      "WARNING: UnitsWarning: 'pixels' did not parse as fits unit: At col 0, Unit 'pixels' not supported by the FITS standard. Did you mean pixel? If this is meant to be a custom unit, define it with 'u.def_unit'. To have it recognized inside a file reader or other code, enable it with 'u.add_enabled_units'. For details, see https://docs.astropy.org/en/latest/units/combining_and_defining.html [astropy.units.core]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading URL https://mast.stsci.edu/api/v0.1/Download/file?uri=mast:TESS/product/tess2020324010417-s0032-0000000140215502-0200-s_lc.fits to ./mastDownload/TESS/tess2020324010417-s0032-0000000140215502-0200-s/tess2020324010417-s0032-0000000140215502-0200-s_lc.fits ... [Done]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: UnitsWarning: 'BJD - 2457000, days' did not parse as fits unit: At col 0, Unit 'BJD' not supported by the FITS standard.  If this is meant to be a custom unit, define it with 'u.def_unit'. To have it recognized inside a file reader or other code, enable it with 'u.add_enabled_units'. For details, see https://docs.astropy.org/en/latest/units/combining_and_defining.html [astropy.units.core]\n",
      "WARNING: UnitsWarning: 'e-/s' did not parse as fits unit: At col 0, Unit 'e' not supported by the FITS standard.  If this is meant to be a custom unit, define it with 'u.def_unit'. To have it recognized inside a file reader or other code, enable it with 'u.add_enabled_units'. For details, see https://docs.astropy.org/en/latest/units/combining_and_defining.html [astropy.units.core]\n",
      "WARNING: UnitsWarning: 'pixels' did not parse as fits unit: At col 0, Unit 'pixels' not supported by the FITS standard. Did you mean pixel? If this is meant to be a custom unit, define it with 'u.def_unit'. To have it recognized inside a file reader or other code, enable it with 'u.add_enabled_units'. For details, see https://docs.astropy.org/en/latest/units/combining_and_defining.html [astropy.units.core]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading URL https://mast.stsci.edu/api/v0.1/Download/file?uri=mast:TESS/product/tess2020351194500-s0033-0000000140215502-0203-s_lc.fits to ./mastDownload/TESS/tess2020351194500-s0033-0000000140215502-0203-s/tess2020351194500-s0033-0000000140215502-0203-s_lc.fits ... [Done]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: UnitsWarning: 'BJD - 2457000, days' did not parse as fits unit: At col 0, Unit 'BJD' not supported by the FITS standard.  If this is meant to be a custom unit, define it with 'u.def_unit'. To have it recognized inside a file reader or other code, enable it with 'u.add_enabled_units'. For details, see https://docs.astropy.org/en/latest/units/combining_and_defining.html [astropy.units.core]\n",
      "WARNING: UnitsWarning: 'e-/s' did not parse as fits unit: At col 0, Unit 'e' not supported by the FITS standard.  If this is meant to be a custom unit, define it with 'u.def_unit'. To have it recognized inside a file reader or other code, enable it with 'u.add_enabled_units'. For details, see https://docs.astropy.org/en/latest/units/combining_and_defining.html [astropy.units.core]\n",
      "WARNING: UnitsWarning: 'pixels' did not parse as fits unit: At col 0, Unit 'pixels' not supported by the FITS standard. Did you mean pixel? If this is meant to be a custom unit, define it with 'u.def_unit'. To have it recognized inside a file reader or other code, enable it with 'u.add_enabled_units'. For details, see https://docs.astropy.org/en/latest/units/combining_and_defining.html [astropy.units.core]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading URL https://mast.stsci.edu/api/v0.1/Download/file?uri=mast:HLSP/qlp/s0087/0000/0001/4021/5502/hlsp_qlp_tess_ffi_s0087-0000000140215502_tess_v01_llc.fits to ./mastDownload/HLSP/hlsp_qlp_tess_ffi_s0087-0000000140215502_tess_v01_llc/hlsp_qlp_tess_ffi_s0087-0000000140215502_tess_v01_llc.fits ... [Done]\n",
      "\n",
      "âœ… Download summary:\n",
      "Sector SPOC FFI ...    Source                                                                File                                                            \n",
      "------ -------- ... ------------ ----------------------------------------------------------------------------------------------------------------------------\n",
      "     5       -- ... TESS-SPOC LC /var/folders/fs/c4gpqhmx5tbcbsv_cbs0yhn80000gn/T/tmplfd_93s4/hlsp_tess-spoc_tess_phot_0000000140215502-s0005_tess_v1_lc.fits\n",
      "     6       -- ... TESS-SPOC LC /var/folders/fs/c4gpqhmx5tbcbsv_cbs0yhn80000gn/T/tmplfd_93s4/hlsp_tess-spoc_tess_phot_0000000140215502-s0006_tess_v1_lc.fits\n",
      "    32       -- ...      SPOC LC         /var/folders/fs/c4gpqhmx5tbcbsv_cbs0yhn80000gn/T/tmplfd_93s4/tess2020324010417-s0032-0000000140215502-0200-s_lc.fits\n",
      "    33       -- ...      SPOC LC         /var/folders/fs/c4gpqhmx5tbcbsv_cbs0yhn80000gn/T/tmplfd_93s4/tess2020351194500-s0033-0000000140215502-0203-s_lc.fits\n",
      "    87       -- ...       QLP LC      /var/folders/fs/c4gpqhmx5tbcbsv_cbs0yhn80000gn/T/tmplfd_93s4/hlsp_qlp_tess_ffi_s0087-0000000140215502_tess_v01_llc.fits\n",
      "\n",
      "Temporary data stored in: /var/folders/fs/c4gpqhmx5tbcbsv_cbs0yhn80000gn/T/tmplfd_93s4\n",
      "This directory will be deleted when the program exits.\n",
      "W :  143\n",
      "W :  143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: UnitsWarning: 'BJD-2457000, days' did not parse as fits unit: At col 0, Unit 'BJD' not supported by the FITS standard.  If this is meant to be a custom unit, define it with 'u.def_unit'. To have it recognized inside a file reader or other code, enable it with 'u.add_enabled_units'. For details, see https://docs.astropy.org/en/latest/units/combining_and_defining.html [astropy.units.core]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W :  144\n",
      "W :  144\n",
      "W :  144\n",
      "W :  144\n",
      "W :  144\n",
      "W :  144\n",
      "W :  144\n",
      "W :  144\n",
      "W :  144\n",
      "W :  144\n",
      "Initial Chi-Sqaured : 183.91 [red 2.87]\n",
      "Fitted parameters for TIC-140215502:\n",
      "t_zero : 2458460.553010982\n",
      "radius_1 : 0.026091091691296253\n",
      "k : 0.05050023389069913\n",
      "b : 0.22332451393667585\n",
      "Final Chi-Sqaured : 86.67 [red 1.35]\n",
      "Initial Chi-Sqaured : 289.37 [red 3.57]\n",
      "Fitted parameters for TIC-140215502:\n",
      "t_zero : 2459194.4394097747\n",
      "radius_1 : 0.037768774276666014\n",
      "k : 0.0651945159211087\n",
      "b : 0.6544319914812814\n",
      "Final Chi-Sqaured : 112.30 [red 1.39]\n",
      "1 49 1 48.9257599195155 733.8863987927325 15 15.289299974848594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:09<00:00,  5.16it/s]\n"
     ]
    }
   ],
   "source": [
    "def func(i):    \n",
    "    # Create the output dir (we'll use this as cache for the data too)\n",
    "    output_dir = os.getcwd() + '/{:}'.format(duos['tic_id'][i])\n",
    "    os.system('mkdir -p {:}'.format(output_dir))\n",
    "    #if os.path.isfile(output_dir + '/' + 'TIC-{:}_ALIASES.png'.format(duos['tic_id'][i])) : return\n",
    "\n",
    "    # Now load the TESS data (SPOC, QLP)\n",
    "    # We are not making our own here like TESSTPF, not yet anyway...\n",
    "    # for data_type\n",
    "    #   single_product -> all sectors together\n",
    "    #   per_sector -> list of per-sector lightcurves\n",
    "    #   northern_duos -> YEARS 2 and 4, then a list of other sectors \n",
    "    #   southern -> YEARS 1 and 3, then a list of other sectors (NOT IMPLEMENTED YET) \n",
    "    t, data,data_labels, base_dir =  bruce.ambiguous_period.download_tess_data(duos['tic_id'][i], \n",
    "                                                              max_sector=None, \n",
    "                                                                   use_ffi=True, \n",
    "                                                                   download_dir=None, \n",
    "                                                                   bin_length=0.5/24)\n",
    "    \n",
    "    # Now flatten the data\n",
    "    for j, k in zip(data, data_labels):\n",
    "        # Flatten the data by SG filter, we need an odd kernel legth based on cadence\n",
    "        j.flatten_data_old(window_width=3, sigmaclip=3, dx_lim=0.1)\n",
    "\n",
    "\n",
    "#         for seg in bruce.data.find_nights_from_data(j.time, dx_lim=0.2):\n",
    "#             j.w = np.ones(j.time.shape[0])*np.median(j.flux)\n",
    "\n",
    "        # Optinally save the data\n",
    "        j.write_data(output_dir + '/' +'TESS_DATA_{:}.txt'.format(k))\n",
    "        fig, ax = j.plot_segments(dx_lim=0.5)\n",
    "        fig.savefig(output_dir + '/' + 'TESS_DATA_{:}.png'.format(k))\n",
    "        plt.close(fig)\n",
    "        \n",
    "        \n",
    "    # Now we need to adjust the time for TTV cases\n",
    "    if duos['tic_id'][i]==118339710:\n",
    "        \n",
    "        x = np.array([1525.64728,2249.54431,2973.53737,3697.59768])+2457000\n",
    "        y = np.array([0,-0.087396,-0.078762,-0.002878])\n",
    "        p_ttv = np.polyfit(x,y, 3)\n",
    "        def p_ttv(x):\n",
    "            return 0.08*np.cos(1425.64728 + 2*x*np.pi/(2*(2171.9503999999997)))\n",
    "        \n",
    "        for d in data:\n",
    "            #print(np.polyval(p, d.time))\n",
    "            #d.time = d.time - np.polyval(p_ttv, d.time)\n",
    "            d.time = d.time + p_ttv(d.time)\n",
    "    # # Now re-order_datasets based on epochs given\n",
    "    # We will unpack now, (data with transits, and data without)\n",
    "    # We may need to change this for it to work properly (Sam is working on it)\n",
    "    # Its worth noting we can incorparate ground based data here too\n",
    "    # data_from_ground = bruce.ambiguous_period.mono_event.photometry_time_series(time, flux, flux_err, w = norm_model)\n",
    "    # Then this can go in data_other_sectors\n",
    "    data, data_labels = bruce.ambiguous_period.group_data_by_epochs(data, data_labels, duos['t_zero_1'][i], duos['t_zero_2'][i])\n",
    "    data, data_other_sectors = data[0], data[1:]\n",
    "\n",
    "\n",
    "\n",
    "    ############################\n",
    "    # FIT EVENT 1\n",
    "    ############################\n",
    "    # Mask data and create the mono_event object\n",
    "    nmask = 3\n",
    "    mask1 = (data.time > (duos['t_zero_1'][i] - nmask*duos['width_1'][i])) &  (data.time < (duos['t_zero_1'][i] + nmask*duos['width_1'][i]))\n",
    "    #data_event_1 = bruce.ambiguous_period.photometry_time_series(data.time[mask1], data.flux[mask1], data.flux_err[mask1], w=data.w[mask1]) #np.percentile(data.flux[mask1], 50)*np.ones(data.time[mask1].shape[0])\n",
    "    data_event_1 = bruce.ambiguous_period.photometry_time_series(data.time[mask1], data.flux[mask1], median_abs_deviation(data.flux[mask1])*np.ones(mask1.sum(), dtype=np.float64), w=data.w[mask1]) #np.percentile(data.flux[mask1], 50)*np.ones(data.time[mask1].shape[0])\n",
    "    m1 = bruce.ambiguous_period.mono_event(duos['t_zero_1'][i], duos['width_1'][i], duos['depth_1'][i], data_event_1, name='TIC-{:}'.format(duos['tic_id'][i]), median_bin_size = None,convolve_bin_size = None)\n",
    "    \n",
    "    # Fit the event and report plots\n",
    "    fig_initial, ax_initial, fig_final, ax_final, return_data_1 = m1.fit_event_with_fixed_period(fit_period=30., plot=True, )\n",
    "    fig_initial.tight_layout()\n",
    "    fig_final.tight_layout()\n",
    "    fig_initial.savefig(output_dir + '/' + 'TIC-{:}_EVENT_1_INITIAL_INITIAL.png'.format(duos['tic_id'][i]))\n",
    "    fig_final.savefig(output_dir + '/' + 'TIC-{:}_EVENT_1_INITIAL_FINAL.png'.format(duos['tic_id'][i]))\n",
    "    plt.close(fig_initial); plt.close(fig_final)\n",
    "\n",
    "\n",
    "    ############################\n",
    "    # FIT EVENT 2\n",
    "    ############################\n",
    "    # Mask data and create the mono_event object\n",
    "    mask2 = (data.time > (duos['t_zero_2'][i] - nmask*duos['width_2'][i])) &  (data.time < (duos['t_zero_2'][i] + nmask*duos['width_2'][i]))\n",
    "    data_event_2 = bruce.ambiguous_period.photometry_time_series(data.time[mask2], data.flux[mask2], data.flux_err[mask2], w=data.w[mask2]) #np.percentile(data.flux[mask2], 50)*np.ones(data.time[mask2].shape[0])\n",
    "    data_event_2 = bruce.ambiguous_period.photometry_time_series(data.time[mask2], data.flux[mask2], median_abs_deviation(data.flux[mask2])*np.ones(mask2.sum(), dtype=np.float64), w=data.w[mask2]) #np.percentile(data.flux[mask2], 50)*np.ones(data.time[mask2].shape[0])\n",
    "    m2 = bruce.ambiguous_period.mono_event(duos['t_zero_2'][i], duos['width_2'][i], duos['depth_2'][i], data_event_2, name='TIC-{:}'.format(duos['tic_id'][i]), median_bin_size = None,convolve_bin_size = 3)\n",
    "\n",
    "    # Fit the event and report plots\n",
    "    fig_initial, ax_initial, fig_final, ax_final, return_data_2 = m2.fit_event_with_fixed_period(fit_period=30., plot=True, )\n",
    "    fig_initial.tight_layout()\n",
    "    fig_final.tight_layout()\n",
    "    fig_initial.savefig(output_dir + '/' + 'TIC-{:}_EVENT_2_INITIAL_INITIAL.png'.format(duos['tic_id'][i]))\n",
    "    fig_final.savefig(output_dir + '/' + 'TIC-{:}_EVENT_2_INITIAL_FINAL.png'.format(duos['tic_id'][i]))\n",
    "    plt.close(fig_initial); plt.close(fig_final)\n",
    "\n",
    "    # We are going to make a nice plot of the two events with their models\n",
    "    fig, ax = plt.subplots(1,2, gridspec_kw={'hspace' : 0, 'wspace' : 0}, figsize = (6.4, 3.8))\n",
    "    ax[0].errorbar(return_data_1[0], return_data_1[1], yerr=return_data_1[2], fmt='k.', alpha = 0.1)\n",
    "    ax[0].plot(return_data_1[3], return_data_1[4], c='orange')\n",
    "    ax[1].errorbar(return_data_2[0], return_data_2[1], yerr=return_data_2[2], fmt='k.', alpha = 0.1)\n",
    "    ax[1].plot(return_data_2[3], return_data_2[4], c='orange')\n",
    "    ax[1].set(yticks=[])\n",
    "    ylim1 = ax[0].get_ylim()\n",
    "    ylim2 = ax[1].get_ylim()\n",
    "    ylim = [min(ylim1[0],ylim2[0]), max(ylim1[1], ylim2[1])]\n",
    "    ax[0].set_ylim(ylim)\n",
    "    ax[1].set_ylim(ylim)\n",
    "    fig.supxlabel('Time from Transit [d]', fontsize=18, x=0.55, y = -0.005)\n",
    "    fig.supylabel('Flux', fontsize=18)\n",
    "    fig.suptitle(m2.name, y=0.95, x=0.55, bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3', alpha=1.0), ha='center', fontsize=18)\n",
    "    plt.subplots_adjust(right=0.99, top=0.99, bottom=0.13)\n",
    "    fig.savefig(output_dir + '/' + 'TIC-{:}_BOTH_EVENTS.png'.format(duos['tic_id'][i]))\n",
    "    plt.close(fig)\n",
    "\n",
    "    ########################################################\n",
    "    # CREATE THE AMBIGUOUS PERIOD OBJECT\n",
    "    ########################################################\n",
    "    p = bruce.ambiguous_period.ambiguous_period(data, events=[m1,m2], name='TIC-{:}'.format(duos['tic_id'][i]),\n",
    "                        median_bin_size = 2,convolve_bin_size = 2)\n",
    "\n",
    "    # Now mask and filter \n",
    "    p.mask_and_filter_events()\n",
    "\n",
    "    # Calculate aliases\n",
    "    # Do not use nsolutions_events here (that is superceeded later)\n",
    "    nsolutions_events = p.calcualte_aliases(dx_lim=0.03, min_period=15)\n",
    "\n",
    "    # Now calcualte whether we saw a transit by comparing the model to a flat line\n",
    "    delta_L_data = p.calcualte_data_delta_L(data)\n",
    "    \n",
    "    \n",
    "    ########################################################\n",
    "    # CHECK NGTS PHOTOMETRY\n",
    "    ########################################################\n",
    "    ngts_data = []\n",
    "    ngts_data_labels = []\n",
    "    if os.path.isfile('ngts_data/TIC-{:}.fits'.format(duos['tic_id'][i])):\n",
    "        ngts = Table.read('ngts_data/TIC-{:}.fits'.format(duos['tic_id'][i]), hdu=4)\n",
    "        ngts.sort('BJD')\n",
    "        mask = (ngts['SIGMA_XS']<0.02) & ~np.isnan(np.array(ngts['FLUX_SYSREM_ALIGNED'], dtype=np.float64)) &\\\n",
    "                     ~np.isnan(np.array(ngts['FLUX_ERR'], dtype=np.float64)) &\\\n",
    "                     ~np.isinf(np.array(ngts['FLUX_SYSREM_ALIGNED'], dtype=np.float64)) &\\\n",
    "                     ~np.isinf(np.array(ngts['FLUX_ERR'], dtype=np.float64))\n",
    "        ngts = ngts[mask]\n",
    "        \n",
    "        # Now lets bin\n",
    "        t_bin, f_bin, fe_bin, c = bruce.data.bin_data(np.array(ngts['BJD'], dtype=np.float64),\n",
    "                                                  np.array(ngts['FLUX_SYSREM_ALIGNED'], dtype=np.float64),\n",
    "                                                  0.5/24/3)\n",
    "\n",
    "        t_bin, f_bin, fe_bin = t_bin[c>10], f_bin[c>10], fe_bin[c>10]\n",
    "        if len(t_bin)>3 : \n",
    "            \n",
    "            #######################################################################################\n",
    "            #    OK, there are two ways of doing this based on data volume\n",
    "            # We could \n",
    "            # 1. Split each night into its own LC so we can see how ruling out aliases\n",
    "            #    progresses. This can be cumbersome and make large plots.\n",
    "            # 2. Treat the NGTS LC as one, but if wee see a transit we will have \n",
    "            #    to do a retrospective calculation to see what aliases is was compatible with.\n",
    "            #    It also might bork the delta_L calculation if a transit is seen but later excluded.\n",
    "            #    Furthermore, if we only have a short amount of in-transit data, we have to be\n",
    "            #    careful about how we detrend.\n",
    "            #######################################################################################\n",
    "\n",
    "            \n",
    "#             #######################################################################################\n",
    "#             # Aproach 1\n",
    "#             #######################################################################################\n",
    "\n",
    "#             # Now create the data products \n",
    "#             for seg in bruce.data.find_nights_from_data(t_bin, dx_lim=0.2): \n",
    "#                 # Now check the phase is consistent with one of the aliases\n",
    "\n",
    "#                 # Now get the phases of all aliases\n",
    "#                 phases = np.zeros((p.aliases.shape[0], len(np.array(t_bin, dtype=np.float64))))\n",
    "#                 for j in range(len(phases)):\n",
    "#                     phases[j] = bruce.data.phase_times(t_bin, p.events[0].de_get_epoch(), p.max_period/p.aliases[j], phase_offset=0.2)\n",
    "#                 phase_widths = p.events[0].de_transit_width() / (p.max_period/p.aliases)\n",
    "\n",
    "#                 # Now mask the data which doesent fall within 1 width of any alias\n",
    "#                 useful_data = np.abs(phases) < (phase_widths[:,np.newaxis]/2) # Make for a nice plot\n",
    "\n",
    "#                 # If so, lets make a data object for the night \n",
    "#                 if useful_data.any():\n",
    "#                     # Now create the time series object\n",
    "#                     d = bruce.ambiguous_period.photometry_time_series(t_bin[seg], f_bin[seg], fe_bin[seg])\n",
    "#                     d.w = np.ones(len(d.time))\n",
    "\n",
    "#                     # Now normalise to the median of the top 20%\n",
    "#                     try:\n",
    "#                         quarter_edges = np.linspace(np.min(d.time)-1e-3,np.max(d.time)+1e-3,5)\n",
    "#                         dig = np.digitize(d.time, quarter_edges, right=True)\n",
    "#                         medians = np.array([np.nanmedian(d.flux[dig==j]) for j in range(1,len(quarter_edges))])\n",
    "#                         constant = np.nanmax(medians)\n",
    "#                     except :  constant = np.nanmedian(d.flux)\n",
    "\n",
    "\n",
    "#                     d.flux = d.flux / constant\n",
    "#                     d.flux_err = d.flux_err / constant\n",
    "#                     ngts_data.append(d)\n",
    "\n",
    "#                     ngts_data_labels.append(Time(d.time[0], format='jd').datetime.strftime('%Y-%m-%d'))            \n",
    "#                     print(Time(d.time[0], format='jd').datetime.strftime('%Y-%m-%d'))\n",
    "\n",
    "\n",
    "            #######################################################################################\n",
    "            # Aproach 2\n",
    "            #######################################################################################\n",
    "            # Now create the time series object\n",
    "            d = bruce.ambiguous_period.photometry_time_series(t_bin, f_bin, fe_bin)\n",
    "            d.w = np.ones(len(d.time))\n",
    "            \n",
    "            # We will do a crude variability removal\n",
    "            # BE CAREFUL, only a small amount of in-transit data will \n",
    "            for seg in bruce.data.find_nights_from_data(d.time, dx_lim=0.2): \n",
    "                d.w[seg] = np.nanmedian(d.flux[seg])*np.ones(seg.shape[0])\n",
    "            ngts_data.append(d)\n",
    "            ngts_data_labels.append('NGTS data')\n",
    "            \n",
    "        # Now we need to adjust the time for TTV cases\n",
    "        if duos['tic_id'][i]==118339710:\n",
    "            for d in ngts_data:\n",
    "                #print(len(d.time), d.time)\n",
    "                #print(np.polyval(p, d.time))\n",
    "                #d.time = d.time - np.polyval(p_ttv, d.time)\n",
    "                d.time = d.time + p_ttv(d.time)\n",
    "    \n",
    "    ########################################################\n",
    "    # CALCULATE DELTA L\n",
    "    ########################################################\n",
    "    #delta_L_data = delta_L_data + 100 # THIS FUDGE IS OFTEN NEEDED\n",
    "#     for j in range(delta_L_data.shape[0]):\n",
    "#         print(j, delta_L_data[j])\n",
    "    delta_L_data_from_other_sectors_or_others = [p.calcualte_data_delta_L(j) for j in data_other_sectors]\n",
    "    delta_L_data_from_ngts = [p.calcualte_data_delta_L(j) for j in ngts_data]\n",
    "    p.delta_L = np.array([delta_L_data, *delta_L_data_from_other_sectors_or_others, *delta_L_data_from_ngts])\n",
    "\n",
    "    ########################################################\n",
    "    # Plot the aliases\n",
    "    ########################################################\n",
    "    fig, ax  = p.plot_aliases(phot_data=[*data_other_sectors, *ngts_data], \n",
    "                              phot_data_labels=[*data_labels, *ngts_data_labels])\n",
    "    fig.savefig(output_dir + '/' + 'TIC-{:}_ALIASES.png'.format(duos['tic_id'][i]), dpi=300)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    ########################################################\n",
    "    # Now report the aliases\n",
    "    ########################################################\n",
    "    aliases = (p.aliases)[p.alias_mask[:,-1]==p.alias_mask.max()]\n",
    "    periods = (p.max_period/p.aliases)[p.alias_mask[:,-1]==p.alias_mask.max()]\n",
    "    alaises = Table()\n",
    "    alaises.add_column(Column(duos['tic_id'][i]*np.ones(len(aliases), dtype=int), name='tic_id'))\n",
    "    alaises.add_column(Column(m1.de_get_epoch()*np.ones(len(aliases)), name='t_zero_1'))\n",
    "    alaises.add_column(Column(m1.de_get_radius_1()*np.ones(len(aliases)), name='radius_1_1'))\n",
    "    alaises.add_column(Column(m1.de_get_k()*np.ones(len(aliases)), name='k_1'))\n",
    "    alaises.add_column(Column(m1.de_get_b()*np.ones(len(aliases)), name='b_1'))\n",
    "    alaises.add_column(Column(m1.de_transit_width()*np.ones(len(aliases)), name='width_1'))\n",
    "    alaises.add_column(Column(m2.de_get_epoch()*np.ones(len(aliases)), name='t_zero_2'))\n",
    "    alaises.add_column(Column(m2.de_get_radius_1()*np.ones(len(aliases)), name='radius_1_2'))\n",
    "    alaises.add_column(Column(m2.de_get_k()*np.ones(len(aliases)), name='k_2'))\n",
    "    alaises.add_column(Column(m2.de_get_b()*np.ones(len(aliases)), name='b_2'))\n",
    "    alaises.add_column(Column(m2.de_transit_width()*np.ones(len(aliases)), name='width_2'))\n",
    "    alaises.add_column(Column(aliases, name='alias'))\n",
    "    alaises.add_column(Column(periods, name='period'))\n",
    "    alaises.write(output_dir + '/' + 'TIC-{:}_ALIASES.fits'.format(duos['tic_id'][i]), overwrite=True)\n",
    "\n",
    "    \n",
    "    ########################################################\n",
    "    # Now plan the aliases\n",
    "    ########################################################\n",
    "#     transit_events = p.transit_plan(start=Time.now(), end = Time.now()+TimeDelta(30, format='jd'), resolution = 1*u.minute,\n",
    "#                     tic_id=duos['tic_id'][i], observatory='Paranal',\n",
    "#                     sun_max_alt=-15, target_min_alt=30, moon_min_seperation=20,\n",
    "#                     min_time_in_transit=None, min_frac_in_transit=None)\n",
    "#     transit_events.add_column(Column(np.ones(len(transit_events))*m1.depth, name='transit_depth_1'))\n",
    "#     transit_events.add_column(Column(np.ones(len(transit_events))*m2.depth, name='transit_depth_2'))\n",
    "#     transit_events.add_column(Column(np.ones(len(transit_events))*m1.width, name='transit_width_1'))\n",
    "#     transit_events.add_column(Column(np.ones(len(transit_events))*m2.width, name='transit_width_2'))\n",
    "#     transit_events.write(output_dir + '/' + 'TIC-{:}_ALIASES_WINDOWS_PARANAL.fits'.format(duos['tic_id'][i]), overwrite=True)\n",
    "#     p.plot_all_events(transit_events, output_dir=output_dir)\n",
    "\n",
    "    \n",
    "    plt.close()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# import multiprocess\n",
    "# with multiprocess.Pool(8) as p : p.map(func, range(len(duos)))\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "# f = open('failed.txt', 'w+')\n",
    "# f.close()\n",
    "# for i in range(len(duos))[5:]: \n",
    "#     try : func(i)\n",
    "#     except: \n",
    "#         with open('failed.txt', 'a+') as f:\n",
    "#             f.write('{:}\\n'.format(duos['tic_id'][i]))\n",
    "\n",
    "            \n",
    "# print(np.argwhere(duos['tic_id']==126565211)[0][0])   \n",
    "func( np.argwhere(duos['tic_id']==140215502)[0][0] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb7fb423",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fs/c4gpqhmx5tbcbsv_cbs0yhn80000gn/T/ipykernel_13389/391604064.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1afb201",
   "metadata": {},
   "source": [
    "# Now look for solved systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a835e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob('*/TIC-*_ALIASES.fits')\n",
    "\n",
    "os.system('mkdir -p solved_systems; rm solved_systems/*')\n",
    "\n",
    "for i in range(len(files)):\n",
    "    tic_id  = int(files[i].split('/')[0])\n",
    "    t = Table.read(files[i])\n",
    "    if len(t)==1:\n",
    "        os.system('cp {:}/TIC-{:}_ALIASES.png solved_systems'.format(tic_id,tic_id))\n",
    "        \n",
    "        print('{:}, {:}, {:}'.format(tic_id,  t['t_zero_1'][0], t['period'][0]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3f1f3bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ngts_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fs/c4gpqhmx5tbcbsv_cbs0yhn80000gn/T/ipykernel_13389/1131739810.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mngts_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ngts_data' is not defined"
     ]
    }
   ],
   "source": [
    "ngts_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b21c8",
   "metadata": {},
   "source": [
    "# Now look for ones with no aliases left (to debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2edde5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385925604\n",
      "231637303\n",
      "118339710\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "files = glob.glob('*/TIC-*_ALIASES.fits')\n",
    "\n",
    "os.system('mkdir -p no_aliases; rm no_aliases/*')\n",
    "\n",
    "for i in range(len(files)):\n",
    "    tic_id  = int(files[i].split('/')[0])\n",
    "    t = Table.read(files[i])\n",
    "    if len(t)==0:\n",
    "        os.system('cp {:}/TIC-{:}_ALIASES.png no_aliases'.format(tic_id,tic_id))\n",
    "        print(tic_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce17cc25",
   "metadata": {},
   "source": [
    "# Now make predictions about when they will transit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f81cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table, vstack\n",
    "import glob\n",
    "\n",
    "# Read all files and combine them\n",
    "files = glob.glob('*/TIC-*_ALIASES_WINDOWS_PARANAL.fits')\n",
    "events = vstack([Table.read(f) for f in files], join_type='outer')\n",
    "\n",
    "# Group by tic_id and night\n",
    "grouped = events.group_by(['tic_id', 'night'])\n",
    "\n",
    "rows = []\n",
    "for g in grouped.groups:\n",
    "    # Start with the first row's values for all columns\n",
    "    row = {col: g[col][0] for col in events.colnames}\n",
    "\n",
    "    # Combine aliasP and aliasPer values as comma-separated strings\n",
    "    row['aliasP'] = ','.join(str(v) for v in g['aliasP'])\n",
    "    row['aliasPer'] = ','.join('{:.5f}'.format(v) for v in g['aliasPer'])\n",
    "    \n",
    "    rows.append(row)\n",
    "\n",
    "# Create summary table\n",
    "summary = Table(rows)\n",
    "summary.sort('night')\n",
    "print(summary['night', 'tic_id', 'aliasP', 'aliasPer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e75c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = summary[summary['night']=='2025-10-29']\n",
    "p = bruce.ambiguous_period.ambiguous_period('a', events=['a','a'], name='dummy',median_bin_size = 2,convolve_bin_size = 2)\n",
    "p.plot_all_events(t, output_dir='transit_plan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f316a21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
