{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9847369",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e19f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/anaconda3/lib/python3.9/site-packages/lightkurve/config/__init__.py:119: UserWarning: The default Lightkurve cache directory, used by download(), etc., has been moved to /Users/sam/.lightkurve/cache. Please move all the files in the legacy directory /Users/sam/.lightkurve-cache to the new location and remove the legacy directory. Refer to https://docs.lightkurve.org/reference/config.html#default-cache-directory-migration for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import bruce, numpy as np, matplotlib.pyplot as plt, os\n",
    "from astropy.table import Table, Column\n",
    "from astropy.stats import sigma_clip\n",
    "from astropy.time import Time , TimeDelta\n",
    "from astropy import units as u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0606724",
   "metadata": {},
   "source": [
    "# Load the tics\n",
    "\n",
    "Data should have the form of (ascii or CSV, your choice). t_zero is in full BJD, width is in days, depth is in normalised flux. If you ue SPOCFIT, if you fit a single transit you will see 3 reported values on the bottom row which are these values for each event (t_zero, width, depth).\n",
    "\n",
    "tic_id,\tt_zero_1,\twidth_1,\tdepth_1,\tt_zero_2,\twidth_2,\tdepth_2,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ebbe077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  TIC ID  Linked TIC ID ...   tic_id \n",
      "--------- ------------- ... ---------\n",
      "  7145074       7145074 ...   7145074\n",
      " 20904104      20904104 ...  20904104\n",
      " 22317640      22317640 ...  22317640\n",
      " 32179255      32179255 ...  32179255\n",
      " 42428568      42428568 ...  42428568\n",
      " 52195587      52195587 ...  52195587\n",
      " 61760996      61760996 ...  61760996\n",
      " 67599025      67599025 ...  67599025\n",
      " 71028120      71028120 ...  71028120\n",
      " 71272316      71272316 ...  71272316\n",
      " 71316629      71316629 ...  71316629\n",
      " 77465954      77465954 ...  77465954\n",
      "107113345     107113345 ... 107113345\n",
      "118339710     118339710 ... 118339710\n",
      "      ...           ... ...       ...\n",
      "115861501     115861501 ... 115861501\n",
      "257024338     257024338 ... 257024338\n",
      "287137785     287137785 ... 287137785\n",
      "284595117     284595117 ... 284595117\n",
      "130714841     130714841 ... 130714841\n",
      " 16982769      16982769 ...  16982769\n",
      "262880382     262880382 ... 262880382\n",
      "157365307     157365307 ... 157365307\n",
      " 39904176      39904176 ...  39904176\n",
      "116261487     116261487 ... 116261487\n",
      "  5267885       5267885 ...   5267885\n",
      "275267824     275267824 ... 275267824\n",
      "286969201     286969201 ... 286969201\n",
      "103095888     103095888 ... 103095888\n",
      "Length = 84 rows\n"
     ]
    }
   ],
   "source": [
    "duos = Table.read('duos.csv')\n",
    "duos['tic_id'] = duos['TIC ID']\n",
    "print(duos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae682c",
   "metadata": {},
   "source": [
    "# Now the main worker function\n",
    "\n",
    "This function does the following:\n",
    "-  Load the latest TESS data\n",
    "-  Flatten the lightcurve\n",
    "-  Fit the events\n",
    "-  Calcualte the aliases\n",
    "-  Plot the permissable aliases\n",
    "-  Create a report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e4891b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(duos))[:1]:\n",
    "    \n",
    "#     if duos['tic_id'][i]!=107113345 : continue\n",
    "    \n",
    "    # Create the output dir (we'll use this as cache for the data too)\n",
    "    output_dir = os.getcwd() + '/{:}'.format(duos['tic_id'][i])\n",
    "    os.system('mkdir -p {:}'.format(output_dir))\n",
    "#     if os.path.isfile(output_dir + '/' + 'TIC-{:}_ALIASES.png'.format(duos['tic_id'][i])) : continue\n",
    "\n",
    "    # Now load the TESS data (SPOC, QLP)\n",
    "    # We are not making our own here like TESSTPF, not yet anyway...\n",
    "    # for data_type\n",
    "    #   single_product -> all sectors together\n",
    "    #   per_sector -> list of per-sector lightcurves\n",
    "    #   northern_duos -> YEARS 2 and 4, then a list of other sectors \n",
    "    #   southern -> YEARS 1 and 3, then a list of other sectors (NOT IMPLEMENTED YET) \n",
    "    t, data,data_labels, base_dir =  bruce.ambiguous_period.download_tess_data(duos['tic_id'][i], \n",
    "                                                              max_sector=None, \n",
    "                                                                   use_ffi=True, \n",
    "                                                                   download_dir=None, \n",
    "                                                                   bin_length=0.5/24)\n",
    "    \n",
    "    # Now flatten the data\n",
    "    for j, k in zip(data, data_labels):\n",
    "        # Flatten the data by SG filter, we need an odd kernel legth based on cadence\n",
    "        j.flatten_data_old(window_width=3, sigmaclip=3, dx_lim=0.1)\n",
    "\n",
    "\n",
    "#         for seg in bruce.data.find_nights_from_data(j.time, dx_lim=0.2):\n",
    "#             j.w = np.ones(j.time.shape[0])*np.median(j.flux)\n",
    "\n",
    "        # Optinally save the data\n",
    "        j.write_data(output_dir + '/' +'TESS_DATA_{:}.txt'.format(k))\n",
    "        fig, ax = j.plot_segments(dx_lim=0.5)\n",
    "        fig.savefig(output_dir + '/' + 'TESS_DATA_{:}.png'.format(k))\n",
    "        plt.close(fig)\n",
    "\n",
    "    # # Now re-order_datasets based on epochs given\n",
    "    # We will unpack now, (data with transits, and data without)\n",
    "    # We may need to change this for it to work properly (Sam is working on it)\n",
    "    # Its worth noting we can incorparate ground based data here too\n",
    "    # data_from_ground = bruce.ambiguous_period.mono_event.photometry_time_series(time, flux, flux_err, w = norm_model)\n",
    "    # Then this can go in data_other_sectors\n",
    "    data, data_labels = bruce.ambiguous_period.group_data_by_epochs(data, data_labels, duos['t_zero_1'][i], duos['t_zero_2'][i])\n",
    "    data, data_other_sectors = data[0], data[1:]\n",
    "\n",
    "\n",
    "\n",
    "    ############################\n",
    "    # FIT EVENT 1\n",
    "    ############################\n",
    "    # Mask data and create the mono_event object\n",
    "    nmask = 3\n",
    "    mask1 = (data.time > (duos['t_zero_1'][i] - nmask*duos['width_1'][i])) &  (data.time < (duos['t_zero_1'][i] + nmask*duos['width_1'][i]))\n",
    "    data_event_1 = bruce.ambiguous_period.photometry_time_series(data.time[mask1], data.flux[mask1], data.flux_err[mask1], w=data.w[mask1]) #np.percentile(data.flux[mask1], 50)*np.ones(data.time[mask1].shape[0])\n",
    "    m1 = bruce.ambiguous_period.mono_event(duos['t_zero_1'][i], duos['width_1'][i], duos['depth_1'][i], data_event_1, name='TIC-{:}'.format(duos['tic_id'][i]), median_bin_size = None,convolve_bin_size = None)\n",
    "    \n",
    "    # Fit the event and report plots\n",
    "    fig_initial, ax_initial, fig_final, ax_final, return_data_1 = m1.fit_event_with_fixed_period(fit_period=30., plot=True, )\n",
    "    fig_initial.tight_layout()\n",
    "    fig_final.tight_layout()\n",
    "    fig_initial.savefig(output_dir + '/' + 'TIC-{:}_EVENT_1_INITIAL_INITIAL.png'.format(duos['tic_id'][i]))\n",
    "    fig_final.savefig(output_dir + '/' + 'TIC-{:}_EVENT_1_INITIAL_FINAL.png'.format(duos['tic_id'][i]))\n",
    "    plt.close(fig_initial); plt.close(fig_final)\n",
    "\n",
    "\n",
    "    ############################\n",
    "    # FIT EVENT 2\n",
    "    ############################\n",
    "    # Mask data and create the mono_event object\n",
    "    mask2 = (data.time > (duos['t_zero_2'][i] - nmask*duos['width_2'][i])) &  (data.time < (duos['t_zero_2'][i] + nmask*duos['width_2'][i]))\n",
    "    data_event_2 = bruce.ambiguous_period.photometry_time_series(data.time[mask2], data.flux[mask2], data.flux_err[mask2], w=data.w[mask2]) #np.percentile(data.flux[mask2], 50)*np.ones(data.time[mask2].shape[0])\n",
    "    m2 = bruce.ambiguous_period.mono_event(duos['t_zero_2'][i], duos['width_2'][i], duos['depth_2'][i], data_event_2, name='TIC-{:}'.format(duos['tic_id'][i]), median_bin_size = None,convolve_bin_size = 3)\n",
    "\n",
    "    # Fit the event and report plots\n",
    "    fig_initial, ax_initial, fig_final, ax_final, return_data_2 = m2.fit_event_with_fixed_period(fit_period=30., plot=True, )\n",
    "    fig_initial.tight_layout()\n",
    "    fig_final.tight_layout()\n",
    "    fig_initial.savefig(output_dir + '/' + 'TIC-{:}_EVENT_2_INITIAL_INITIAL.png'.format(duos['tic_id'][i]))\n",
    "    fig_final.savefig(output_dir + '/' + 'TIC-{:}_EVENT_2_INITIAL_FINAL.png'.format(duos['tic_id'][i]))\n",
    "    plt.close(fig_initial); plt.close(fig_final)\n",
    "\n",
    "    # We are going to make a nice plot of the two events with their models\n",
    "    fig, ax = plt.subplots(1,2, gridspec_kw={'hspace' : 0, 'wspace' : 0}, figsize = (6.4, 3.8))\n",
    "    ax[0].errorbar(return_data_1[0], return_data_1[1], yerr=return_data_1[2], fmt='k.', alpha = 0.1)\n",
    "    ax[0].plot(return_data_1[3], return_data_1[4], c='orange')\n",
    "    ax[1].errorbar(return_data_2[0], return_data_2[1], yerr=return_data_2[2], fmt='k.', alpha = 0.1)\n",
    "    ax[1].plot(return_data_2[3], return_data_2[4], c='orange')\n",
    "    ax[1].set(yticks=[])\n",
    "    ylim1 = ax[0].get_ylim()\n",
    "    ylim2 = ax[1].get_ylim()\n",
    "    ylim = [min(ylim1[0],ylim2[0]), max(ylim1[1], ylim2[1])]\n",
    "    ax[0].set_ylim(ylim)\n",
    "    ax[1].set_ylim(ylim)\n",
    "    fig.supxlabel('Time from Transit [d]', fontsize=18, x=0.55, y = -0.005)\n",
    "    fig.supylabel('Flux', fontsize=18)\n",
    "    fig.suptitle(m2.name, y=0.95, x=0.55, bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3', alpha=1.0), ha='center', fontsize=18)\n",
    "    plt.subplots_adjust(right=0.99, top=0.99, bottom=0.13)\n",
    "    fig.savefig(output_dir + '/' + 'TIC-{:}_BOTH_EVENTS.png'.format(duos['tic_id'][i]))\n",
    "    plt.close(fig)\n",
    "\n",
    "    ########################################################\n",
    "    # CREATE THE AMBIGUOUS PERIOD OBJECT\n",
    "    ########################################################\n",
    "    p = bruce.ambiguous_period.ambiguous_period(data, events=[m1,m2], name='TIC-{:}'.format(duos['tic_id'][i]),\n",
    "                        median_bin_size = 2,convolve_bin_size = 2)\n",
    "\n",
    "    # Now mask and filter \n",
    "    p.mask_and_filter_events()\n",
    "\n",
    "    # Calculate aliases\n",
    "    # Do not use nsolutions_events here (that is superceeded later)\n",
    "    nsolutions_events = p.calcualte_aliases(dx_lim=0.03, min_period=15)\n",
    "\n",
    "    # Now calcualte whether we saw a transit by comparing the model to a flat line\n",
    "    delta_L_data = p.calcualte_data_delta_L(data)\n",
    "    \n",
    "    \n",
    "    ########################################################\n",
    "    # CHECK NGTS PHOTOMETRY\n",
    "    ########################################################\n",
    "    ngts_data = []\n",
    "    ngts_data_labels = []\n",
    "    if os.path.isfile('ngts_data/TIC-{:}.fits'.format(duos['tic_id'][i])):\n",
    "        ngts = Table.read('ngts_data/TIC-{:}.fits'.format(duos['tic_id'][i]), hdu=4)\n",
    "        ngts.sort('BJD')\n",
    "        mask = (ngts['SIGMA_XS']<0.02) & ~np.isnan(np.array(ngts['FLUX_SYSREM_ALIGNED'], dtype=np.float64)) &\\\n",
    "                     ~np.isnan(np.array(ngts['FLUX_ERR'], dtype=np.float64)) &\\\n",
    "                     ~np.isinf(np.array(ngts['FLUX_SYSREM_ALIGNED'], dtype=np.float64)) &\\\n",
    "                     ~np.isinf(np.array(ngts['FLUX_ERR'], dtype=np.float64))\n",
    "        ngts = ngts[mask]\n",
    "        \n",
    "        # Now lets bin\n",
    "        t_bin, f_bin, fe_bin, c = bruce.data.bin_data(np.array(ngts['BJD'], dtype=np.float64),\n",
    "                                                  np.array(ngts['FLUX_SYSREM_ALIGNED'], dtype=np.float64),\n",
    "                                                  0.5/24/3)\n",
    "\n",
    "        t_bin, f_bin, fe_bin = t_bin[c>10], f_bin[c>10], fe_bin[c>10]\n",
    "        if len(t_bin)>3 : \n",
    "            \n",
    "            #######################################################################################\n",
    "            #    OK, there are two ways of doing this based on data volume\n",
    "            # We could \n",
    "            # 1. Split each night into its own LC so we can see how ruling out aliases\n",
    "            #    progresses. This can be cumbersome and make large plots.\n",
    "            # 2. Treat the NGTS LC as one, but if wee see a transit we will have \n",
    "            #    to do a retrospective calculation to see what aliases is was compatible with.\n",
    "            #    It also might bork the delta_L calculation if a transit is seen but later excluded.\n",
    "            #    Furthermore, if we only have a short amount of in-transit data, we have to be\n",
    "            #    careful about how we detrend.\n",
    "            #######################################################################################\n",
    "\n",
    "            \n",
    "#             #######################################################################################\n",
    "#             # Aproach 1\n",
    "#             #######################################################################################\n",
    "\n",
    "#             # Now create the data products \n",
    "#             for seg in bruce.data.find_nights_from_data(t_bin, dx_lim=0.2): \n",
    "#                 # Now check the phase is consistent with one of the aliases\n",
    "\n",
    "#                 # Now get the phases of all aliases\n",
    "#                 phases = np.zeros((p.aliases.shape[0], len(np.array(t_bin, dtype=np.float64))))\n",
    "#                 for j in range(len(phases)):\n",
    "#                     phases[j] = bruce.data.phase_times(t_bin, p.events[0].de_get_epoch(), p.max_period/p.aliases[j], phase_offset=0.2)\n",
    "#                 phase_widths = p.events[0].de_transit_width() / (p.max_period/p.aliases)\n",
    "\n",
    "#                 # Now mask the data which doesent fall within 1 width of any alias\n",
    "#                 useful_data = np.abs(phases) < (phase_widths[:,np.newaxis]/2) # Make for a nice plot\n",
    "\n",
    "#                 # If so, lets make a data object for the night \n",
    "#                 if useful_data.any():\n",
    "#                     # Now create the time series object\n",
    "#                     d = bruce.ambiguous_period.photometry_time_series(t_bin[seg], f_bin[seg], fe_bin[seg])\n",
    "#                     d.w = np.ones(len(d.time))\n",
    "\n",
    "#                     # Now normalise to the median of the top 20%\n",
    "#                     try:\n",
    "#                         quarter_edges = np.linspace(np.min(d.time)-1e-3,np.max(d.time)+1e-3,5)\n",
    "#                         dig = np.digitize(d.time, quarter_edges, right=True)\n",
    "#                         medians = np.array([np.nanmedian(d.flux[dig==j]) for j in range(1,len(quarter_edges))])\n",
    "#                         constant = np.nanmax(medians)\n",
    "#                     except :  constant = np.nanmedian(d.flux)\n",
    "\n",
    "\n",
    "#                     d.flux = d.flux / constant\n",
    "#                     d.flux_err = d.flux_err / constant\n",
    "#                     ngts_data.append(d)\n",
    "\n",
    "#                     ngts_data_labels.append(Time(d.time[0], format='jd').datetime.strftime('%Y-%m-%d'))            \n",
    "#                     print(Time(d.time[0], format='jd').datetime.strftime('%Y-%m-%d'))\n",
    "\n",
    "\n",
    "            #######################################################################################\n",
    "            # Aproach 2\n",
    "            #######################################################################################\n",
    "            # Now create the time series object\n",
    "            d = bruce.ambiguous_period.photometry_time_series(t_bin, f_bin, fe_bin)\n",
    "            d.w = np.ones(len(d.time))\n",
    "            \n",
    "            # We will do a crude variability removal\n",
    "            # BE CAREFUL, only a small amount of in-transit data will \n",
    "            for seg in bruce.data.find_nights_from_data(d.time, dx_lim=0.2): \n",
    "                d.w[seg] = np.nanmedian(d.flux[seg])*np.ones(seg.shape[0])\n",
    "            ngts_data.append(d)\n",
    "            ngts_data_labels.append('NGTS data')\n",
    "    \n",
    "    ########################################################\n",
    "    # CALCULATE DELTA L\n",
    "    ########################################################\n",
    "    #delta_L_data = delta_L_data + 100 # THIS FUDGE IS OFTEN NEEDED\n",
    "#     for j in range(delta_L_data.shape[0]):\n",
    "#         print(j, delta_L_data[j])\n",
    "    delta_L_data_from_other_sectors_or_others = [p.calcualte_data_delta_L(j) for j in data_other_sectors]\n",
    "    delta_L_data_from_ngts = [p.calcualte_data_delta_L(j) for j in ngts_data]\n",
    "    p.delta_L = np.array([delta_L_data, *delta_L_data_from_other_sectors_or_others, *delta_L_data_from_ngts])\n",
    "\n",
    "    ########################################################\n",
    "    # Plot the aliases\n",
    "    ########################################################\n",
    "    fig, ax  = p.plot_aliases(phot_data=[*data_other_sectors, *ngts_data], \n",
    "                              phot_data_labels=[*data_labels, *ngts_data_labels])\n",
    "    fig.savefig(output_dir + '/' + 'TIC-{:}_ALIASES.png'.format(duos['tic_id'][i]), dpi=300)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    ########################################################\n",
    "    # Now report the aliases\n",
    "    ########################################################\n",
    "    aliases = (p.aliases)[p.alias_mask[:,-1]==p.alias_mask.max()]\n",
    "    periods = (p.max_period/p.aliases)[p.alias_mask[:,-1]==p.alias_mask.max()]\n",
    "    alaises = Table()\n",
    "    alaises.add_column(Column(duos['tic_id'][i]*np.ones(len(aliases), dtype=int), name='tic_id'))\n",
    "    alaises.add_column(Column(m1.de_get_epoch()*np.ones(len(aliases)), name='t_zero_1'))\n",
    "    alaises.add_column(Column(m1.de_get_radius_1()*np.ones(len(aliases)), name='radius_1_1'))\n",
    "    alaises.add_column(Column(m1.de_get_k()*np.ones(len(aliases)), name='k_1'))\n",
    "    alaises.add_column(Column(m1.de_get_b()*np.ones(len(aliases)), name='b_1'))\n",
    "    alaises.add_column(Column(m1.de_transit_width()*np.ones(len(aliases)), name='width_1'))\n",
    "    alaises.add_column(Column(m2.de_get_epoch()*np.ones(len(aliases)), name='t_zero_2'))\n",
    "    alaises.add_column(Column(m2.de_get_radius_1()*np.ones(len(aliases)), name='radius_1_2'))\n",
    "    alaises.add_column(Column(m2.de_get_k()*np.ones(len(aliases)), name='k_2'))\n",
    "    alaises.add_column(Column(m2.de_get_b()*np.ones(len(aliases)), name='b_2'))\n",
    "    alaises.add_column(Column(m2.de_transit_width()*np.ones(len(aliases)), name='width_2'))\n",
    "    alaises.add_column(Column(aliases, name='alias'))\n",
    "    alaises.add_column(Column(periods, name='period'))\n",
    "    alaises.write(output_dir + '/' + 'TIC-{:}_ALIASES.fits'.format(duos['tic_id'][i]), overwrite=True)\n",
    "\n",
    "    \n",
    "    ########################################################\n",
    "    # Now plan the aliases\n",
    "    ########################################################\n",
    "    transit_events = p.transit_plan(start=Time.now(), end = Time.now()+TimeDelta(30, format='jd'), resolution = 1*u.minute,\n",
    "                    tic_id=duos['tic_id'][i], observatory='Paranal',\n",
    "                    sun_max_alt=-15, target_min_alt=30, moon_min_seperation=20,\n",
    "                    min_time_in_transit=None, min_frac_in_transit=None)\n",
    "    transit_events.write(output_dir + '/' + 'TIC-{:}_ALIASES_WINDOWS_PARANAL.fits'.format(duos['tic_id'][i]), overwrite=True)\n",
    "    p.plot_all_events(transit_event, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aee262",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir + '/' + 'TIC-{:}_ALIASES.png'.format(duos['tic_id'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32985561",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([*data_labels, *ngts_data_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6032f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "            # Now normalise to the median of the top 20%\n",
    "            quarter_edges = np.linspace(np.min(d.time)-1e-3,np.max(d.time)+1e-3,5)\n",
    "            dig = np.digitize(d.time, quarter_edges, right=True)\n",
    "            medians = np.array([np.nanmedian(d.flux[dig==i]) for i in range(1,len(quarter_edges))])\n",
    "            constant = np.max(medians)\n",
    "            d.flux = d.flux / constant\n",
    "            d.flux_err = d.flux_err / constant\n",
    "            ngts_data.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1afb201",
   "metadata": {},
   "source": [
    "# Now lok for solved systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a835e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224279805, 2458369.727329248, 22.984009901774698\n",
      "52195587, 2458350.2770604594, 47.33320196554996\n",
      "356158613, 2458713.0097145545, 21.388638748029503\n",
      "257024338, 2458922.4813573235, 81.43377512902953\n",
      "396720998, 2458399.237195914, 46.35443516011583\n",
      "229476204, 2458743.522001057, 648.058725417126\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "files = glob.glob('*/TIC-*_ALIASES.fits')\n",
    "\n",
    "os.system('mkdir -p solved_systems; rm solved_systems/*')\n",
    "\n",
    "for i in range(len(files)):\n",
    "    tic_id  = int(files[i].split('/')[0])\n",
    "    t = Table.read(files[i])\n",
    "    if len(t)==1:\n",
    "        os.system('cp {:}/TIC-{:}_ALIASES.png solved_systems'.format(tic_id,tic_id))\n",
    "        \n",
    "        print('{:}, {:}, {:}'.format(tic_id,  t['t_zero_1'][0], t['period'][0]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob('*/TIC-*_ALIASES.fits')\n",
    "\n",
    "os.system('mkdir -p solved_systems; rm solved_systems/*')\n",
    "\n",
    "for i in range(len(files)):\n",
    "    tic_id  = int(files[i].split('/')[0])\n",
    "    t = Table.read(files[i])\n",
    "    if len(t)==1:\n",
    "        os.system('cp {:}/TIC-{:}_ALIASES.png solved_systems'.format(tic_id,tic_id))\n",
    "        \n",
    "        print('{:}, {:}, {:}'.format(tic_id,  t['t_zero_1'][0], t['period'][0]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce17cc25",
   "metadata": {},
   "source": [
    "# Now make predictions about when they will transit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901750cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  night    ...\n",
      "---------- ...\n",
      "2025-10-23 ...\n",
      "2025-10-23 ...\n",
      "2025-10-23 ...\n",
      "2025-10-23 ...\n",
      "2025-10-23 ...\n",
      "2025-10-23 ...\n",
      "2025-10-23 ...\n",
      "2025-10-24 ...\n",
      "2025-10-24 ...\n",
      "2025-10-24 ...\n",
      "2025-10-24 ...\n",
      "2025-10-24 ...\n",
      "2025-10-24 ...\n",
      "2025-10-25 ...\n",
      "       ... ...\n",
      "2025-11-20 ...\n",
      "2025-11-20 ...\n",
      "2025-11-20 ...\n",
      "2025-11-20 ...\n",
      "2025-11-20 ...\n",
      "2025-11-20 ...\n",
      "2025-11-20 ...\n",
      "2025-11-20 ...\n",
      "2025-11-21 ...\n",
      "2025-11-21 ...\n",
      "2025-11-21 ...\n",
      "2025-11-21 ...\n",
      "2025-11-21 ...\n",
      "2025-11-21 ...\n",
      "Length = 204 rows\n"
     ]
    }
   ],
   "source": [
    "from astropy.table import Table, vstack\n",
    "import glob\n",
    "\n",
    "# Read all files and combine them\n",
    "files = glob.glob('*/TIC-*_ALIASES_WINDOWS_PARANAL.fits')\n",
    "events = vstack([Table.read(f) for f in files], join_type='outer')\n",
    "\n",
    "# Group by tic_id and night\n",
    "grouped = events.group_by(['tic_id', 'night'])\n",
    "\n",
    "rows = []\n",
    "for g in grouped.groups:\n",
    "    # Start with the first row's values for all columns\n",
    "    row = {col: g[col][0] for col in events.colnames}\n",
    "\n",
    "    # Combine aliasP and aliasPer values as comma-separated strings\n",
    "    row['aliasP'] = ','.join(str(v) for v in g['aliasP'])\n",
    "    row['aliasPer'] = ','.join(str(v) for v in g['aliasPer'])\n",
    "    \n",
    "    rows.append(row)\n",
    "\n",
    "# Create summary table\n",
    "summary = Table(rows)\n",
    "summary.sort('night')\n",
    "print(summary['night', 'tic_id', 'aliasP', 'aliasPer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e75c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = summary[summary['night']=='2025-10-28']\n",
    "p = bruce.ambiguous_period.ambiguous_period('a', events=['a','a'], name='dummy',median_bin_size = 2,convolve_bin_size = 2)\n",
    "p.plot_all_events(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f52680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586f5f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3604b38b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
